{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c0d5f67",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "# Siamese Network Tutorial\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/WillzWayn/growing-happy/AI/AI/Siameses Tower/ContrastiveLoss - Pytorch - Siameses Tower.ipynb)\n",
    "\n",
    "\n",
    "\n",
    "googlecolab/colabtools/blob/main/notebooks/colab-github-demo.ipynb\n",
    "\n",
    "In this notebook i will walk through the creation of a Siamese Neural Network, which is often used for tasks like image similarity (signature verification, face matching, ...), text similarity, and one-shot learning.\n",
    "\n",
    "In this case, we will use the MNIST dataset to build the network, which will compare pairs of images and determine whether they belong to the same class or not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaae4fad",
   "metadata": {},
   "source": [
    "## Step 1: Import Required Libraries\n",
    "We will start by importing necessary libraries for data manipulation, neural networks, and visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0dc1004",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import random\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.datasets import MNIST\n",
    "from torch import optim\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b1fd61",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Setup Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "543e8b58",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "EPOCHS = 15\n",
    "EMBEDDING_SPACE = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e868e6a",
   "metadata": {},
   "source": [
    "## Step 2: Set Up Device\n",
    "Here, we define whether we will be using a GPU (CUDA) or CPU for our computations. If a GPU is available, it will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca10bb0d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11dfdc99",
   "metadata": {},
   "source": [
    "## Step 3: Create a Custom Dataset for Siamese Networks\n",
    "We will now create a custom dataset class `SiameseDataset`. This dataset will provide pairs of images from the MNIST dataset, where each pair will either belong to the same class or to different classes. The label will indicate whether the images are similar (0 for same class, 1 for different classes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797b2731",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class SiameseDataset(Dataset):\n",
    "    def __init__(self, data, transform=None):\n",
    "        \"\"\"\n",
    "        Initializes the SiameseDataset class.\n",
    "        \n",
    "        Args:\n",
    "            data: List of tuples where each tuple contains (image, label).\n",
    "            transform: Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.data = data  # Store the dataset.\n",
    "        self.transform = transform  # Store the image transformation function.\n",
    "\n",
    "    def _get_image_from_same_class(self, labelA):\n",
    "        labelB = -1\n",
    "        while labelB != labelA:\n",
    "            imgB, labelB = random.choice(self.data)\n",
    "        return imgB, labelB\n",
    "    \n",
    "    def _get_image_from_different_class(self, labelA):\n",
    "        labelB = labelA\n",
    "        while labelB == labelA:\n",
    "            imgB, labelB = random.choice(self.data)\n",
    "        return imgB, labelB\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        Returns a sample from the dataset, consisting of two images and a label indicating\n",
    "        if they belong to the same class.\n",
    "        \n",
    "        Args:\n",
    "            index: Index of the first image in the dataset.\n",
    "        \n",
    "        Returns:\n",
    "            imgA, imgB: A pair of images.\n",
    "            label: A binary tensor indicating whether the images are from the same class (0) \n",
    "                   or different classes (1).\n",
    "        \"\"\"\n",
    "        imgA, labelA = self.data[index]\n",
    "        same_class_flag = random.randint(0, 1)\n",
    "\n",
    "        if same_class_flag:\n",
    "            imgB, labelB = self._get_image_from_same_class(labelA)\n",
    "        else:\n",
    "            imgB, labelB = self._get_image_from_different_class(labelA)\n",
    "\n",
    "        if self.transform:\n",
    "            imgA = self.transform(imgA)\n",
    "            imgB = self.transform(imgB)\n",
    "\n",
    "        return imgA, imgB, torch.tensor([labelA != labelB], dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "\n",
    "mnist_train = MNIST(root='./data', train=True, download=True)\n",
    "mnist_test = MNIST(root='./data', train=False, download=True)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomRotation(degrees=15)\n",
    "])\n",
    "\n",
    "siamese_train = SiameseDataset(mnist_train, transform)\n",
    "siamese_test = SiameseDataset(mnist_test, transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f879bc34",
   "metadata": {},
   "source": [
    "## Step 4: Visualize the Dataset\n",
    "Let's visualize a few samples of image pairs from our custom `SiameseDataset`. This will help us understand how the pairs are generated (same class or different class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0ba10ff",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'siamese_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 31\u001b[0m\n\u001b[0;32m     28\u001b[0m     plt\u001b[38;5;241m.\u001b[39msavefig(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mplot_siamese_samples.png\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Example usage:\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m plot_siamese_samples(\u001b[43msiamese_train\u001b[49m, num_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'siamese_train' is not defined"
     ]
    }
   ],
   "source": [
    "def plot_siamese_samples(dataset, num_samples=5):\n",
    "    \"\"\"\n",
    "    Plots pairs of images from the SiameseDataset along with their label indicating\n",
    "    if they belong to the same class or different classes.\n",
    "    \n",
    "    Args:\n",
    "        dataset: An instance of the SiameseDataset.\n",
    "        num_samples: Number of image pairs to plot.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(num_samples, 2, figsize=(8, num_samples * 2))\n",
    "    fig.suptitle('Siamese Dataset Image Pairs')\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        imgA, imgB, label = dataset[random.randint(0, len(dataset)-1)]\n",
    "        imgA = imgA.cpu().squeeze().numpy()\n",
    "        imgB = imgB.cpu().squeeze().numpy()\n",
    "\n",
    "        axes[i, 0].imshow(imgA, cmap='gray')\n",
    "        axes[i, 0].axis('off')\n",
    "        axes[i, 0].set_title(f'Image A - LABEL: {label}')\n",
    "\n",
    "        axes[i, 1].imshow(imgB, cmap='gray')\n",
    "        axes[i, 1].axis('off')\n",
    "        axes[i, 1].set_title('Image B')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.85)\n",
    "    plt.savefig('plot_siamese_samples.png')\n",
    "\n",
    "# Example usage:\n",
    "plot_siamese_samples(siamese_train, num_samples=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b6e187",
   "metadata": {},
   "source": [
    "## Step 5: Build the Siamese Network Architecture\n",
    "The `SiameseNetwork` consists of two branches that share weights. These branches process the two input images, and the results are compared to determine the similarity. The architecture consists of convolutional layers followed by fully connected layers.\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9787403",
   "metadata": {
    "lines_to_next_cell": 1,
    "title": "Tower definition"
   },
   "outputs": [],
   "source": [
    "class Tower(nn.Module):\n",
    "    \"\"\"\n",
    "    This is the embedding generator.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(Tower, self).__init__()\n",
    "\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(256 * 3 * 3, 1024),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Linear(1024, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Linear(256, EMBEDDING_SPACE)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = self.cnn(x)\n",
    "        output = output.view(output.size()[0], -1)\n",
    "        output = self.fc(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a78f79",
   "metadata": {
    "lines_to_next_cell": 1,
    "title": "SiameseNetwork definition"
   },
   "outputs": [],
   "source": [
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        self.tower = Tower()  # Shared tower network\n",
    "\n",
    "    def forward(self, inputA, inputB):\n",
    "        outputA = self.tower(inputA)\n",
    "        outputB = self.tower(inputB)\n",
    "        return outputA, outputB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75fb7829",
   "metadata": {},
   "source": [
    "## Step 6: Define the Contrastive Loss Function\n",
    "We use the contrastive loss function to optimize the Siamese Network. This loss function computes the distance between two outputs and penalizes the network based on whether they are from the same class or not.\n",
    "\n",
    "- **The margin forces samples from different classes to be at least a certain distance apart**; if the euclidean_distance < margin, the difference is positive, penalizing the model, with the penalty increasing as the distance decreases.\n",
    "- **A lower loss** indicates that embeddings of the same class are close together, and embeddings of different classes are appropriately separated.\n",
    "- **A higher loss** would indicate that embeddings of the same class are far apart or that embeddings of different classes are too close."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1dc36871",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(torch.nn.Module):\n",
    "    def __init__(self, margin=3.0):\n",
    "        # The margin is the threshold\n",
    "        # that determines how far apart the embeddings\n",
    "        # of different classes should be.\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, outputA, outputB, label):\n",
    "        # The forward method defines the computation of the contrastive loss.\n",
    "        # It receives two outputs (outputA, outputB) and a label (1 or 0).\n",
    "        # 'outputA' and 'outputB' represent the embeddings for two input samples.\n",
    "\n",
    "        # Compute the Euclidean distance between the two outputs (embeddings).\n",
    "        euclidean_distance = F.pairwise_distance(outputA, outputB, keepdim=True)\n",
    "\n",
    "        # Loss for pairs that belong to the same class (label = 0). This encourages embeddings\n",
    "        # from the same class to be close to each other by minimizing the squared distance.\n",
    "        same_class_loss = (1 - label) * (euclidean_distance ** 2)\n",
    "\n",
    "        # Loss for pairs that belong to different classes (label = 1). This encourages embeddings\n",
    "        # from different classes to be separated by at least 'margin' by penalizing distances \n",
    "        # below the margin. The clamp ensures the difference doesn't go below 0.\n",
    "        diff_class_loss = (label) * (torch.clamp(self.margin - euclidean_distance, min=0.0) ** 2)\n",
    "\n",
    "        # Return the average loss across the batch.\n",
    "        return torch.mean(same_class_loss + diff_class_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04c59c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.6546)\n"
     ]
    }
   ],
   "source": [
    "# Example of input in mock data\n",
    "# Two embeddings of size (batch_size, embedding_dimension)\n",
    "outputA = torch.tensor([[0.5, 1.0], [0.8, 0.6], [1.5, 1.3]])  # Embeddings for the first set of samples\n",
    "outputB = torch.tensor([[0.4, 0.9], [0.2, 0.1], [1.4, 1.2]])  # Embeddings for the second set of samples\n",
    "\n",
    "# Labels indicating if they belong to the same class or not\n",
    "# 0 means same class, 1 means different class\n",
    "label = torch.tensor([[0], [1], [0]])  # Batch size = 3\n",
    "\n",
    "# Calculate the loss\n",
    "criterion = ContrastiveLoss()\n",
    "loss = criterion(outputA, outputB, label)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48562550",
   "metadata": {},
   "source": [
    "## Step 7: Training the Network\n",
    "Now we will train the network using the `SiameseDataset` and the `ContrastiveLoss`. The training loop involves computing the loss for each image pair and updating the model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de055172",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(siamese_train, shuffle=True, batch_size=64)\n",
    "\n",
    "model = SiameseNetwork().to(device)\n",
    "criterion = ContrastiveLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    \n",
    "    for imgA, imgB, label in tqdm(train_dataloader, desc=f\"Train epoch: {epoch+1}\"):\n",
    "        imgA, imgB, label = imgA.to(device), imgB.to(device), label.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputA, outputB = model(imgA, imgB)\n",
    "        loss_contrastive = criterion(outputA, outputB, label)\n",
    "        loss_contrastive.backward()\n",
    "\n",
    "        total_loss += loss_contrastive.item()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch}; Loss {total_loss}\")\n",
    "\n",
    "def evaluate_model(model, dataloader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgA, imgB, label in dataloader:\n",
    "            imgA, imgB, label = imgA.to(device), imgB.to(device), label.to(device)\n",
    "            outputA, outputB = model(imgA, imgB)\n",
    "            euclidean_distance = F.pairwise_distance(outputA, outputB)\n",
    "            predictions = euclidean_distance > 1.0  # Threshold for same/different class\n",
    "            total_correct += (predictions == label).sum().item()\n",
    "            total_samples += label.size(0)\n",
    "\n",
    "    accuracy = total_correct / total_samples\n",
    "    return accuracy\n",
    "\n",
    "test_dataloader = DataLoader(siamese_test, shuffle=False, batch_size=64)\n",
    "accuracy = evaluate_model(model, test_dataloader)\n",
    "print(f'Test Dataset Accuracy: {accuracy}%')\n",
    "\n",
    "def plot_siamese_predict_samples(dataset, num_samples=5):\n",
    "    \"\"\"\n",
    "    Plots pairs of images from the SiameseDataset along with their label indicating\n",
    "    if they belong to the same class or different classes.\n",
    "    \n",
    "    Args:\n",
    "        dataset: An instance of the SiameseDataset.\n",
    "        num_samples: Number of image pairs to plot.\n",
    "    \"\"\"\n",
    "    # Set up the plot with 2 columns for image pairs\n",
    "    fig, axes = plt.subplots(num_samples, 2, figsize=(8, num_samples * 2))\n",
    "    fig.suptitle('Siamese Dataset Image Pairs')\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        # Randomly pick an index from the dataset\n",
    "        imgA, imgB, label = dataset[random.randint(0, len(dataset)-1)]\n",
    "        imgA, imgB, label = imgA.to(device), imgB.to(device), label.to(device)\n",
    "        outputA, outputB = model(imgA.unsqueeze(0), imgB.unsqueeze(0))\n",
    "        similarity_score = F.pairwise_distance(outputA, outputB).cpu().item()\n",
    "\n",
    "        # Convert tensors to numpy arrays for plotting\n",
    "        # Move tensors to CPU before converting to NumPy arrays\n",
    "        imgA = imgA.cpu().squeeze().numpy()  # Changed line\n",
    "        imgB = imgB.cpu().squeeze().numpy()  # Changed line\n",
    "\n",
    "        # Display first image (imgA)\n",
    "        axes[i, 0].imshow(imgA, cmap='gray')\n",
    "        axes[i, 0].axis('off')\n",
    "        axes[i, 0].set_title(f'SCORE: {similarity_score}')\n",
    "        PRED_LABEL = 'Same Label' if similarity_score < .35 else 'DIFF Label'\n",
    "        # Display second image (imgB)\n",
    "        axes[i, 1].imshow(imgB, cmap='gray')\n",
    "        axes[i, 1].axis('off')\n",
    "        axes[i, 1].set_title(f'PREDICT: {PRED_LABEL}')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.85)\n",
    "    plt.savefig('plot_siamese_predict_samples.png')\n",
    "\n",
    "\n",
    "plot_siamese_predict_samples(siamese_test, num_samples=5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9251611b",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac64972",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvar o modelo da Tower\n",
    "torch.save(model.tower, 'tower_model_siamese.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef01ddf1",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Step 8: Load the Network\n",
    "Let's test the network on a few image pairs from the test dataset and visualize the similarity scores.\n",
    "\n",
    "```python\n",
    "import torch.nn as nn\n",
    "from torchvision.datasets import MNIST\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f13edd0",
   "metadata": {
    "title": "Tower definition"
   },
   "outputs": [],
   "source": [
    "tower_model = torch.load('tower_model_siamese.pth', map_location=device)\n",
    "tower_model.eval()  # Eval mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03142ef",
   "metadata": {},
   "source": [
    "## Step 8: Test the Network\n",
    "Let's test the network on a few image pairs from the test dataset and visualize the similarity scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af927097",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_test = MNIST(root='./data', train=False, download=True)\n",
    "test_tensor = torch.utils.data.TensorDataset(torch.tensor(mnist_test.data), torch.tensor(mnist_test.targets))\n",
    "# Create a data loader from the tensor dataset\n",
    "test_loader = torch.utils.data.DataLoader(test_tensor, batch_size=1, shuffle=False)\n",
    "\n",
    "\n",
    "# EVAL USANDO TOWER:\n",
    "for imgA, label in test_tensor:\n",
    "    imgA = imgA.type(torch.float32).unsqueeze(0).unsqueeze(0).to(device)  \n",
    "    outputA = tower_model(imgA)\n",
    "    print(outputA)\n",
    "    break\n",
    "\n",
    "for imgA, label in test_loader:\n",
    "    imgA = imgA.type(torch.float32).unsqueeze(0).to(device)  \n",
    "    outputA = tower_model(imgA)\n",
    "    print(outputA)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d76d7fe-24c6-437b-8385-5ff0b8a13bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Visualize the embeddings.\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Define a function to extract embeddings from the tower model\n",
    "# Create a data loader from the tensor dataset\n",
    "# Define a function to extract embeddings from the tower model\n",
    "def extract_embeddings(loader, model, device):\n",
    "    embeddings = []\n",
    "    labels = []\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.type(torch.float32)\n",
    "            x = x.to(device)\n",
    "            output = model(x.unsqueeze(0))\n",
    "            embeddings.append(output.cpu().numpy())\n",
    "            labels.append(y.cpu().numpy())\n",
    "    return np.concatenate(embeddings), np.concatenate(labels)\n",
    "\n",
    "embeddings, labels = extract_embeddings(test_loader, tower_model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b27531-0232-49af-a11f-e419234a90cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use t-SNE to reduce the dimensionality of the embeddings\n",
    "def visualizate_embeddings(embeddings, labels, annotation_sample=.06):\n",
    "    # Use t-SNE to reduce the dimensionality of the embeddings\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    embeddings_2d = tsne.fit_transform(embeddings)\n",
    "\n",
    "    # Plot the embedding space\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], c=labels)\n",
    "\n",
    "    # Sample x% of the labels for annotation\n",
    "    sample_idx = np.random.choice(range(len(labels)), size=int(annotation_sample * len(labels)), replace=False)\n",
    "    for i in sample_idx:\n",
    "        x, y = embeddings_2d[i, 0], embeddings_2d[i, 1]\n",
    "        plt.annotate(str(labels[i]), (x, y), textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
    "\n",
    "    plt.title('MNIST Embedding Space')\n",
    "    plt.xlabel('Dimension 1')\n",
    "    plt.ylabel('Dimension 2')\n",
    "    plt.colorbar()\n",
    "    plt.savefig('ContrastiveLoss - MNIST Embedding Space.png')\n",
    "\n",
    "visualizate_embeddings(embeddings, labels)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "title,-all",
   "encoding": "# -*- coding: utf-8 -*-",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
